<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="Super FF&#39; Blogs">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Super FF&#39; Blogs">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Super FF&#39; Blogs">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>Super FF' Blogs</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Super FF' Blogs</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">STAY HUNGRY. STAY FOOLISH</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/20/hello-world/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Frankie">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Super FF' Blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/20/hello-world/" itemprop="url">Hello World</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-20T10:08:56+08:00">
                2018-10-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/20/Tree-Model-in-ML/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Frankie">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Super FF' Blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/20/Tree-Model-in-ML/" itemprop="url">Tree Model in ML</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-20T16:44:29+08:00">
                2018-04-20
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/study/" itemprop="url" rel="index">
                    <span itemprop="name">study</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="tree-model-in-machine-learning">Tree Model in Machine Learning</h1>
<p>Hope somebody have heard about Decision Tree, CART, Random Forest, GBDT, et.al. Actually, all of those algorithms are based on Tree. How could tree would help computes to do classification or regression tasks? Let’s explore it:</p>
<h2 id="define-task">Define Task</h2>
<p>Before exploring all of those tree models, let’s define the task which we would handle by using above algorithms.</p>
<h4 id="classification-task">Classification Task</h4>
<p>Suppose, we want to predict whether a person like playing computer games according to gender, age, occupation, et.al.</p>
<p>Then the problem set is: Given <span class="math inline">\(X\in F_{m\times n}, Y \in {like, dislike}\)</span> to predict person whether like computer games.</p>
<h4 id="regression-task">Regression Task</h4>
<p>Nowadays in China, the house pricing is the most concerned topic. Suppose we have a lot of house parameters such as size, the distance btw mall, et.al. Then we need to predict houses’ price according to historic data.</p>
<p>Then the problem set: Given <span class="math inline">\(X\in F_{m\times n}, Y\in R_{1\times n}\)</span> to build a model to predict house’s price according to <span class="math inline">\(X,Y\)</span></p>
<h2 id="decision-tree">Decision Tree</h2>
<p>Let’s look at the following picture(copy from <a href="http://xgboost.readthedocs.io/en/latest/model.html" target="_blank" rel="noopener">xgboost</a>):</p>
<figure>
<img src="https://raw.githubusercontent.com/dmlc/web-data/master/xgboost/model/cart.png" alt="img"><figcaption>img</figcaption>
</figure>
<p>Then you would know the process of decision tree. We need to choose a feature to split the whole data sets until the tree node cannot to split. And the critical problem is how to choose the splitting feature at each step.</p>
<h4 id="impurity">Impurity</h4>
<p>Suppose the whole dataset is <strong>impure</strong> due to there are too many classes(like, dislike). At each step, we need to choose a feature which could increase the purity after splitting the dataset according to the selected feature. Because the split node would contain small classes.</p>
<ul>
<li><p>Information Entropy</p>
<p>Entropy is used to define the confusion degree of a system. In information theory, Shannon used it to define the information. The entropy is bigger, the information is more. We could use it to define the impurity. <strong>The entropy is bigger, the impurity is bigger.</strong> <span class="math display">\[
Entropy(X) = \sum_{i=0}^c -p_i \log p_i
\]</span> <span class="math inline">\(p_i = \frac{num\ of \ class\ i}{total\ num}\)</span></p></li>
<li><p>Gini Coefficient</p>
<p>Gini coefficient is used to define the income equality. It is bigger then the income is more inequality. So could also use it to define the impurity. <strong>The gini index is bigger, the impurity is bigger.</strong> <span class="math display">\[
Gini(X) = 1- \sum_{i=1}^{c}p_i^2
\]</span> , <span class="math inline">\(p_i\)</span> is the same as above.</p></li>
</ul>
<h4 id="informationpurity-gain">Information/Purity Gain</h4>
<p>Above we have talked about how to define the impurity. So we split the dataset based on the feature which could most increase the dataset’s purity. No matter entropy or gini index, after splitting they would decrease. So the information gain is: <span class="math display">\[
Gain(X, f) = Impurity\_before - Impurity\_after
\]</span></p>
<p><span class="math display">\[
Impurity\_after = \sum_{v=1}^V \frac{|D_v|}{|D|} impurity\_of\_v
\]</span></p>
<p>So at each step , we choose the biggest Gain feature to split the tree node until reach some condition(depth, leaf node constriction, et.al.). After the tree built, we could use the model to do classification, just like a marble flow from the root node to leafs. Each leaf represents one class/label.</p>
<figure>
<img src="https://cdn-images-1.medium.com/max/1200/1*OuB7IlENrdpXozK1R0UEzw.gif" alt="img"><figcaption>img</figcaption>
</figure>
<p><strong>Note:</strong></p>
<ul>
<li>Let’s think about one index feature, such as there 100 sample, then their index feature is 1,2,…,1001,2,…,100 . The index feature would lead to best split. But it is of course not a good choice. Actually <strong>Information Gain</strong> prefers the more value features.</li>
<li>How could we deal with numerical features? <strong>Bucketing</strong></li>
</ul>
<h4 id="conclusion">Conclusion</h4>
<p>Now let conclude the decision tree algorithm:</p>
<ul>
<li>Compute current impurity (Entropy or Gini Index), <strong>impurity_before</strong></li>
<li>Select a feature and compute the impurity, <strong>impurity_after</strong></li>
<li>Repeat Step 2 for all features</li>
<li>Compute the information gain for all features</li>
<li>Select the biggest information gain feature to split current node</li>
<li>Repeat above steps until cannot be split</li>
</ul>
<h2 id="regression-tree">Regression Tree</h2>
<p>Above we have explored how decision tree do classification tasks. Now we will go through the regression task. Let’s think about the difference between classification and regression. The most important is the class/label. Classification task would have fixed values, but for regression it would be infinity. In decision tree, we could use the fixed classed to compute the impurity and found best split. In regression, how could we split the tree? And how to measure the impurity after splitting?</p>
<p>In regression tree, <strong>we often use mean square error (hereinafter is MSE) to measure the impurity.</strong> Of course, we could use other variants of MSE, such as mean absolute error(hereinafter MAE), friedman_mse, et.al.</p>
<p>Then when do regression, after data points flowed to leaf nodes, the predicted result is the average label of leaf node. That is: <span class="math inline">\(result=average(leaf(y))\)</span></p>
<p><strong>That’s easy!!!</strong></p>
<h2 id="ensemble-learning">Ensemble Learning</h2>
<h4 id="bias-variance-decomposition">Bias-Variance Decomposition</h4>
<p>Define following variables:</p>
<ul>
<li><span class="math inline">\(E(f;D)\)</span> : error between actual value and predicted value.</li>
<li><span class="math inline">\(f(x;D)\)</span> : predicted value on train set <strong>D</strong></li>
<li><span class="math inline">\(f(x)=ED[f(x;D)]\)</span>: expectation of <span class="math inline">\(f(x;D)\)</span> on different datasets</li>
<li><span class="math inline">\(y_D\)</span>: label on train set <strong>D</strong></li>
<li><span class="math inline">\(y\)</span>: actual label of the true distribution for whole data set</li>
</ul>
<p>For regression, we could decompose <span class="math inline">\(E(f;D)\)</span> as follows: <span class="math display">\[
E(f;D) = E_D[(f(x;D)-f(x))^2] + (f(x)-y)^2 + E_D[(y_D-y)^2]
\]</span> So <span class="math inline">\(var = E_d[(f(x;D)-f(x))^2]\)</span> is the variance caused by the limited train set. And <span class="math inline">\(noise = E_D[(y_D-y)^2]\)</span></p>
<p>which caused by data, and we cannot get over it.</p>
<p><span class="math inline">\(bias = (f(x) - y)^2\)</span> is the bias between predicted value of model and actual label.</p>
<p>That is: <span class="math inline">\(E(f;D) = bias + var + noise\)</span></p>
<p>So we could improve our model from two points: <strong>bias</strong> and <strong>var</strong></p>
<h4 id="bias-variance-trade-off">Bias-variance Trade-off</h4>
<p>Intuitively, <em>bias</em> represents the error between model predicted value and the train set’s value. <em>variance</em> indicates the error between train set’s value and the actual value. When <em>bias</em> decreased(model fits train set better), <em>variance</em>would increase. Because when train more, the data quality could effect accuracy more. We called this is <strong>Bias-variance trade-off</strong></p>
<figure>
<img src="http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png" alt="bias variance trade off"><figcaption>bias variance trade off</figcaption>
</figure>
<h4 id="bagging">Bagging</h4>
<p>Bagging is the method which improve models from <strong>var</strong> point of view. It increases the train data sets’ diversity by sub-sampling both on instances and features. The most popular Tree Model bagging algorithm is Random Forest.</p>
<h5 id="random-forest-extremely-randomized-trees">Random Forest &amp; Extremely Randomized Trees</h5>
<p><a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier" target="_blank" rel="noopener">A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control <em>over-fitting</em>.</a> Please refer to the <a href="https://pdfs.semanticscholar.org/presentation/ee84/702c392425e77725017f38c557e97744951d.pdf" target="_blank" rel="noopener">Bias-variance decomposition in Random Forests</a> to get more about why using more models ensemble could decrease the variance.</p>
<figure>
<img src="https://c.mql5.com/2/33/image1__1.png" alt="Random Forest"><figcaption>Random Forest</figcaption>
</figure>
<h4 id="boosting">Boosting</h4>
<p>Until now, we could improve our model’s accuracy by decreasing variance. Are there any methods which could both decrease variance and bias? Of course, boosting could make this dream comes true.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/12/01/Logistic-Regression/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Frankie">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Super FF' Blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/12/01/Logistic-Regression/" itemprop="url">Logistic Regression</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-12-01T16:21:37+08:00">
                2017-12-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/study/" itemprop="url" rel="index">
                    <span itemprop="name">study</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="logistic-regression">Logistic Regression</h1>
<p>This name could make many colleges confused. In fact, it is a linear model for binary classification tasks. Let’s think about how computer could classify two people according to their features(such as height, weight, hair length, et.al). Of course we should use statistical method combining with lots of data. LR is hereinafter referred to as the “Logistic Regression”. LR is one linear model to do classification tasks. It first calculate a result according to a linear model such as <span class="math inline">\(y=w^T∗x\)</span>. Then it regularizes <span class="math inline">\(y\)</span> into space <span class="math inline">\([0,1]\)</span>. And 0, 1 represent two classes. At last, if <span class="math inline">\(y\)</span> is closer to 0, then it belongs to 0. Otherwise 1.</p>
<p>OK. Comes with the question, how do we regularize <span class="math inline">\(y\)</span>? And why we convert it to space <span class="math inline">\([0,1]\)</span> not <span class="math inline">\([−1,1]\)</span>?</p>
<h2 id="explanation-with-commercial-concepts">Explanation with commercial concepts</h2>
<p>Suppose we have the classification task, predict one customer whether to buy a product according to some features, such as salary, gender, age, et.al. Suppose one customer buy product’s probability is <span class="math inline">\(p\)</span>, then <span class="math inline">\(1-p\)</span>is the un-buy probability. Define <span class="math inline">\(q = \frac{p}{1-p}\)</span>, then <span class="math inline">\(q \in (0, +\infty)\)</span>. Now we should find a function to fit <span class="math inline">\(q\)</span>. When pp is 0, it means the people don’t want to buy anything, now <span class="math inline">\(q\)</span> is 0. When <span class="math inline">\(p\)</span> increase a bit, <span class="math inline">\(q\)</span> nearly didn’t increase. When one has strong feeling to buy, <span class="math inline">\(q\)</span> increases rapidly. At last it tends to infinity. The exponential function is the best to describe this characteristics. So we use <span class="math inline">\(e^y\)</span> (refer to following picture)to fit <span class="math inline">\(q\)</span>. Such that <span class="math inline">\(q=\frac{p}{1-p}=e^y\)</span></p>
<figure>
<img src="https://mathbitsnotebook.com/Algebra2/Exponential/exgraph3.jpg" alt="exponetial function"><figcaption>exponetial function</figcaption>
</figure>
<p>Then, <span class="math inline">\(p=\frac{1}{1+e^y}\)</span>. This is the <span class="math inline">\(sigmoid\)</span> function, and this function is the core of logistic regression.</p>
<h2 id="statistical-explanation">Statistical Explanation</h2>
<p>The above explanation is very rigorous. Now from statistic point of view, we can think the binary classification problem as n Bernoulli experiments. That is <span class="math inline">\(X \sim B(n, \theta)\)</span>, then <span class="math inline">\(p(y=1|\theta) = \theta\)</span>, <span class="math inline">\(p(y=0|\theta)=1-\theta\)</span>.</p>
<p>So that <span class="math display">\[
p(y|\theta) = \theta^{y} (1-\theta)^{1-y}
\]</span> And the likelihood function is: <span class="math display">\[
L = \prod_{i=1}^n p^{(i)}
\]</span> According to maximum likelihood estimation: <span class="math inline">\(max(L)\)</span> is equals to <span class="math inline">\(max(\log(L))\)</span></p>
<p>so the objective is: <span class="math display">\[
obj = max(\sum_{i=1}^n(\log p^{(i)}))
\]</span></p>
<p><span class="math display">\[
obj = max(\sum_{i=1}^n (y^{(i)}\log\theta + (1-y^{(i)})\log(1-\theta)))
\]</span></p>
<p><span class="math display">\[
obj = max(\sum_{i=1}^n (\log(1-\theta) + y^{(i)} \log\frac{\theta}{1-\theta}))
\]</span></p>
<p>where <span class="math inline">\(y^{(i)} = W^T x^{(i)}\)</span></p>
<h2 id="generalized-linear-model">Generalized Linear Model</h2>
<p>Above section has done the statistical explanation, but there are two unknown parameters: <span class="math inline">\(W^T\)</span> and <span class="math inline">\(\theta\)</span>. Actually we want the probability <span class="math inline">\(\theta\)</span> could be associated with <span class="math inline">\(x\)</span> and <span class="math inline">\(W^T\)</span>. For detailed explanation, please refer to my previous blog <a href="">General Linear Models</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/10/Linear-Regression/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Frankie">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Super FF' Blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/11/10/Linear-Regression/" itemprop="url">Linear Regression</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-11-10T16:06:20+08:00">
                2017-11-10
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/study/" itemprop="url" rel="index">
                    <span itemprop="name">study</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="linear-regression">Linear Regression</h1>
<p>Linear Regression is the first algorithm when I start to learn machine learning. Before machine learning, I have got in touch with it in statistics. In my memory, linear regression is very simple. And the most important thing is Least Mean Square(hereinafter LMS). But why using LMS to calculate linear regression model? Now we will explore this.</p>
<h2 id="construct-the-problem">Construct the problem</h2>
<p>Given the sample<span class="math inline">\(X\in R_{m \times n}\)</span> (we always use column vector to represent sample data), <span class="math inline">\(Y\in R_{1 \times n}\)</span>. Suppose <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>has the linear relationship, we need to get the model’s parameters <span class="math inline">\(W \in R_{m \times 1}\)</span>. Except that, we would give a <span class="math inline">\(bias\)</span>. So the model would be: <span class="math display">\[
Y = W^{T} * X + bias
\]</span> For simplicity, we could extend <span class="math inline">\(X\)</span> to <span class="math inline">\(R_{m+1 \times n}\)</span>. The last row are all 1. Then extend <span class="math inline">\(W\)</span> to <span class="math inline">\(R_{m+1 \times n}\)</span>, the <span class="math inline">\(bias\)</span> could be merged to <span class="math inline">\(W\)</span>. Now the model becomes: <span class="math display">\[
Y = W^{T} * X
\]</span> An the problem is to calculate the unknown item <span class="math inline">\(W\)</span></p>
<h2 id="theory-analysis">Theory Analysis</h2>
<p>Above we have constructed the problem, due to the sample data always more than parameters. So we cannot get a unique result. But we could use optimization to approximate the result: <span class="math display">\[
Y^{‘} = W^{T} * X
\]</span> , where <span class="math display">\[
Y^{‘} \approx Y
\]</span> Now the LSM occurred, define<span class="math inline">\(L = \frac{1}{2} * (Y^{‘} - Y)^2\)</span> , and minimize <span class="math inline">\(L\)</span> could make <span class="math inline">\(Y^{&#39;}\)</span> approximates to <span class="math inline">\(Y\)</span> . The optimization problem is: <span class="math inline">\(min(L)\)</span>.</p>
<h3 id="statistical-explanation">Statistical Explanation</h3>
<p>From statistical point of view, suppose<span class="math inline">\(y = y^{‘} + \epsilon\)</span>, where <span class="math inline">\(\epsilon\)</span> is the error. In statistic, we define error is Gaussian distribution, by normalization we could get:<span class="math inline">\(\epsilon \sim N(0,\delta)\)</span> . So, <span class="math display">\[
p(\epsilon|W,x) = \frac{1}{\sqrt{2\pi}\delta} \exp(-\frac{\epsilon^2}{2\delta^2}) = \frac{1}{\sqrt{2\pi}\delta} \exp(-\frac{(y-W^Tx)^2}{2\delta^2})
\]</span> For point <span class="math inline">\(i\)</span>, <span class="math inline">\(p(ϵ(i)|W,x(i))=p(y(i)|W,x(i))\)</span>, so the likelihood function: <span class="math display">\[
L(W) = \prod_{i=1}^{n} p(y^{(i)} | W, x^{(i)})
\]</span></p>
<p><span class="math display">\[
\log{L(W)} = \sum_{i=1}^m p(y^{(i)} | x^{(i)},W) = -m\log{\sqrt{2\pi}\delta} - \sum_{i=1}^m \frac{(y^{(i)} - W^Tx^{(i)})^2}{2\delta^2}
\]</span></p>
<p>According to max likelihood approximation: <span class="math display">\[
obj = max(\log L(W))
\]</span> so as to, <span class="math display">\[
obj = \min{(\sum_{i=1}^m (y^{(i)} - W^Tx^{(i)})^2)}
\]</span> This is the same as Least Mean Square.</p>
<h2 id="solve-the-optimization-problem">Solve the Optimization Problem</h2>
<p>In optimization, the gradient decent is the most common used algorithm. It just like people to find a fastest way to get down a hillside. This person would stop to try every directions around him. If one way led to the fastest down way at current location, he would choose it. He would repeat above methods until arrived the destination.</p>
<p>OK. In math, gradient is the fastest rising direction. So we could use negative gradient as the fasted decrease direction. At each step, we compute the gradient of the objective function. Then update the unknown parameters as: <span class="math display">\[
W = W - \eta * gradient(obj)
\]</span> In detail, for the j-th parameter <span class="math inline">\(W_j\)</span> of <span class="math inline">\(W\)</span> , the update rule is: <span class="math display">\[
W_j = W_j - \eta * \frac{\partial(obj)}{\partial(x_j^{(i)})}
\]</span></p>
<p><span class="math display">\[
W_j = W_j - \eta * (W^TX^{(i)} - y^{(i)})x^{(i)}_j
\]</span></p>
<p>For all sample data, the update rule is to sum all the gradient: <span class="math display">\[
W_j = W_j - \sum_{i=1}^{n}\eta * (W^TX^{(i)} - y^{(i)})x^{(i)}_j
\]</span></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/10/20/General-Linear-Model/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Frankie">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Super FF' Blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/10/20/General-Linear-Model/" itemprop="url">General Linear Model</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-10-20T15:04:23+08:00">
                2017-10-20
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/study/" itemprop="url" rel="index">
                    <span itemprop="name">study</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="线性模型-generalize-linear-models">线性模型 – Generalize Linear Models</h1>
<p>所谓的GLMs是一类机器学习算法，其中最小二乘线性回归（LMS）以及逻辑回归（LR）都是GLMs的一个特例。</p>
<h2 id="线性模型的特性">线性模型的特性</h2>
<ol type="1">
<li><h3 id="指数分布族-the-exponential-family">指数分布族 – The exponential family</h3>
<p>所有GLMs的分布律都可以写成如下的指数分布族：<span class="math inline">\(p(y;\eta) = b(y)exp(\eta^TT(y) - a(\eta))\)</span></p>
<p>其中<span class="math inline">\(\eta\)</span>被称为自然参数或者规范参数， <span class="math inline">\(T(y)\)</span> 被称为充分统计量，<span class="math inline">\(a(\eta)\)</span> 是对数分量函数。</p></li>
<li><h3 id="构建glm的假设条件">构建GLM的假设条件</h3>
<ul>
<li><p><span class="math inline">\(y|x;\theta \sim ExponetialFamily(\eta)\)</span></p></li>
<li><p>给定<span class="math inline">\(x\)</span>，我们的目标是预测<span class="math inline">\(T(y)\)</span>的期望值。即希望通过假设函数 <span class="math display">\[
h_{\theta}(x)
\]</span> 预测的结果满足 <span class="math display">\[
h_\theta(x) = E[T(y)|x]
\]</span></p></li>
<li><p>自然参数<span class="math inline">\(\eta\)</span> 与 <span class="math inline">\(x\)</span> 线性相关，即<span class="math inline">\(\eta = \theta^Tx\)</span></p></li>
</ul></li>
</ol>
<h2 id="线性模型实例">线性模型实例</h2>
<p>下面我们介绍一些线性模型的实例，比如我们常见的线性回归，逻辑回归以及Softmax回归多分类器。</p>
<ol type="1">
<li><h3 id="线性回归">线性回归</h3>
<p>线性回归是一种进行连续值预测的常用方法。假设有<span class="math inline">\(m\)</span>个样本,每个样本包含<span class="math inline">\(n\)</span>个特征，即<span class="math inline">\(X\in R^{m,n}\)</span>，那么回归预测的问题可以描述成 <span class="math display">\[
y^{(i)} = \theta^T * x^{(i)} + \epsilon^{(i)}
\]</span> 其中<span class="math inline">\(\theta\)</span>为待拟合的线性模型的权重，<span class="math inline">\(\epsilon(i)\)</span>为拟合误差。</p>
<p>假设各个样本的拟合误差独立同分布于（IID）正态分布，即<span class="math inline">\(\epsilon^{(i)} \sim N(\mu,\delta^2)\)</span> . 假设<span class="math inline">\(\mu = 0\)</span>(可以通过正则化来达到)，那么 <span class="math display">\[
p(\epsilon^i) = \frac{1}{\sqrt{2\pi}\delta}\exp{-\frac{(\epsilon^{(i)})^2}{2\delta^2}}
\]</span></p>
<p><span class="math display">\[
p(y^{(i)} | x^{(i)},\theta) = \frac{1}{\sqrt{2\pi}\delta}\exp{-\frac{(y^{(i)} - \theta^T*x^{(i)})^2}{2\delta^2}}
\]</span></p>
<p>那么，该分布的似然函数为: <span class="math display">\[
L(\theta) = \prod_{i=1}^m p(y^{(i)} | x^{(i)},\theta)
\]</span></p>
<p><span class="math display">\[
\log{L(\theta)} = \sum_{i=1}^m p(y^{(i)} | x^{(i)},\theta) = -m\log{\sqrt{2\pi}\delta} - \sum_{i=1}^m \frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\delta^2}
\]</span></p>
<p>为了使得线性模型能够得到较好的拟合效果,即最大似然估计: <span class="math display">\[
\max{(\log{L(\theta)})}
\]</span> 结合上式，即 <span class="math display">\[
\min{(\sum_{i=1}^m (y^{(i)} - \theta^Tx^{(i)})^2)}
\]</span> 其中无论<span class="math inline">\(\delta\)</span>如何取值都与问题求解无关。 那么最终的拟合过程，变成了最小二乘问题。</p></li>
<li><h3 id="逻辑回归">逻辑回归</h3>
<p>逻辑回归是一种二分类问题，假设给你m个样本，每个样本<span class="math inline">\(n\)</span>个特征，并且所有样本有两个类别，假设是{0,1}。假设样本服从二项分布，即 <span class="math display">\[
p(y=1|\phi) = \phi, p(y=0|\phi) = 1- \phi
\]</span> 那么 <span class="math display">\[
p(y|\phi) = \phi^y(1-\phi)^{1-y}
\]</span></p>
<p><span class="math display">\[
p(y|\phi) = \exp{(y\log{\phi} + (1-y)\log{(1-\phi)})}
\]</span></p>
<p><span class="math display">\[
p(y|\phi) = \exp{(y\log{\frac{\phi}{1-\phi}} + \log{(1-\phi)})}
\]</span></p>
<p>构建GLM得到： <span class="math display">\[
T(y) = y, b(y) = 1, \eta = \log{\frac{\phi}{1-\phi}}, \phi = \frac{1}{1+e^{\eta}}, a(\eta) = \log{(1+e^{\eta})}
\]</span> 又 <span class="math display">\[
\eta^{(i)} = \theta^T * x^{(i)}
\]</span> 所以 <span class="math display">\[
\phi^{(i)} = \frac{1}{1+e^{-(\theta^T x^{(i)})}}
\]</span> 同时，为了求解拟合参数<span class="math inline">\(\theta\)</span>.参考线性回归的求解过程：</p>
<ul>
<li><p>求解似然函数：<span class="math inline">\(L(\theta) = \prod_{i=1}^{m}((\phi^{(i)})^{y^{(i)}} (1-\phi^{(i)})^{(1-y^{(i)})})\)</span></p></li>
<li><p>对似然函数变形：<span class="math inline">\(l(\theta) = \log{L(\theta)} = \sum_{i=1}^{m}(y^{(i)}\log{\phi^{(i)}}+(1-y^{(i)})\log{(1-\phi^{(i)})})\)</span></p></li>
<li><p>求解变形后的似然函数的最大值，梯度下降求解。每一步迭代的梯度为： <span class="math display">\[
\frac{\partial{(l(\theta))}}{\partial{\theta_j}} = (y\frac{1}{\phi} + (1-y)\frac{1}{1-\phi}) \phi(1-\phi)x_j
\]</span></p>
<p><span class="math display">\[
\frac{\partial{(l(\theta))}}{\partial{\theta_j}} = (y - \phi)x_{j}
\]</span></p></li>
</ul></li>
<li><h3 id="softmax回归">Softmax回归</h3>
<p>Softmax Regression主要用于解决多分类问题，假设 <span class="math display">\[
y\in \{1,2,3,…,k\}
\]</span> 用于表示样本的类别。</p>
<p><span class="math inline">\(\phi_i\)</span> 表示样本属于第<span class="math inline">\(i\)</span>类的概率，即<span class="math inline">\(p(y=i;\phi) = \phi_i\)</span></p>
<p>由于<span class="math inline">\(\sum_{i=1}^{k}\phi_i = 1\)</span>, 所以可以令<span class="math inline">\(\phi_{k} = 1-\sum_{i=1}^{k-1}\phi_i\)</span></p>
<p>为了使得多项分布能够用指数函数族描述，在此引入<span class="math inline">\(T(y)\)</span>,其中 <span class="math display">\[
T(1) = \left(\begin{array}{c} 1 \\ 0 \\ 0 \\ \vdots \\ 0 \end{array} \right), T(2) = \left(\begin{array}{c} 0 \\ 1 \\ 0 \\ \vdots \\ 0 \end{array} \right), \ldots, T(k-1) = \left(\begin{array}{c} 0 \\ 0 \\ 0 \\ \vdots \\ 1 \end{array} \right), T(k) = \left(\begin{array}{c} 0 \\ 0 \\ 0 \\ \vdots \\ 0 \end{array} \right)
\]</span> 那么<span class="math inline">\(T(y) \neq y, T(y) \in R^{k-1}\)</span>。且<span class="math inline">\((T(y))_i = 1\{y=i\}\)</span> ，其中<span class="math inline">\(1\{.\}=1\)</span> when · is true.</p>
<p>那么<span class="math inline">\(E[(T(y))_i] = p(y=i) = \phi_i\)</span> <span class="math display">\[
p(y;\phi) = \phi_1^{1\{y=1\}}\phi_2^{1\{y=2\}}\ldots\phi_k^{1\{y=k\}}
\]</span></p>
<p><span class="math display">\[
p(y;\phi) = \exp({1\{y=1\}}\log\{\phi_1\}+\ldots+{1\{y=k\}}\log\{\phi_k\})
\]</span></p>
<p><span class="math display">\[
p(y;\phi) = \exp({1\{y=1\}}\log\{\phi_1\}+\ldots+(1-\sum_{i=1}^{k-1}{1\{y=i\}})\log\{\phi_k\})
\]</span></p>
<p><span class="math display">\[
p(y;\phi) = \exp(\sum_{i=1}^{k-1}{1\{y=i\}}\log\{\phi_i\}+\ldots+(1-\sum_{i=1}^{k-1}{1\{y=i\}})\log\{\phi_k\})
\]</span></p>
<p><span class="math display">\[
p(y;\phi) = \exp(\sum_{i=1}^{k-1}{1\{y=i\}}\log\{\phi_i/\phi_k\}+\ldots+\log{\phi_k})
\]</span></p>
<p><span class="math display">\[
p(y;\phi) = \exp(\sum_{i=1}^{k-1}{(T(y))_i}\log\{\phi_i/\phi_k\}+\ldots+\log{\phi_k})
\]</span></p>
<p><span class="math display">\[
p(y;\eta) = b(y)exp(\eta^TT(y) - a(\eta))
\]</span></p>
<p>得到$b(y)=1, = (\begin{array}{c} (_1/_k) \ (_2/_k) \ (_3/<em>k) \ \ (</em>{k-1}/_k) \end{array} ) <span class="math inline">\(, 其中\)</span>a() = -(_k)$</p>
<p>link function(<span class="math inline">\(\eta\)</span>为分布律的函数)：<span class="math inline">\(\eta_i = \log(\phi_i/\phi_k)\)</span></p>
<p>response function(分布律为<span class="math inline">\(\eta\)</span>的函数，也称为softmax函数)：<span class="math inline">\(\phi_i = e^{\eta_i}/\sum_{j=1}^k{e^{\eta_j}}\)</span></p>
<p>又：<span class="math inline">\(\eta_i=\theta_i^Tx\)</span></p>
<p>得：<span class="math inline">\(\phi_i = \exp(\theta_i^Tx)/\sum_{j=1}^k{\exp(\theta_j^Tx)}\)</span></p>
<p>那么假设函数： <span class="math display">\[
h_{\theta}(x) = E[T(y)|x;\theta] = \left(\begin{array}{c} \phi_1 \\ \phi_2 \\ \phi_3 \\ \vdots \\ \phi_{k-1} \end{array} \right) = \left(\begin{array}{c} \frac{\exp(\theta_1^Tx)}{\sum_{j=1}^k{\exp(\theta_j^Tx)}} \\ \frac{\exp(\theta_2^Tx)}{\sum_{j=1}^k{\exp(\theta_j^Tx)}} \\ \frac{\exp(\theta_3^Tx)}{\sum_{j=1}^k{\exp(\theta_j^Tx)}} \\ \vdots \\ \frac{\exp(\theta_{k-1}^Tx)}{\sum_{j=1}^k{\exp(\theta_j^Tx)}} \end{array} \right)
\]</span> 似然函数为： <span class="math display">\[
l(\theta) = \sum_{i=1}^{m}\log(p(y^{(i)}|x^{(i)};\theta)) = \sum_{i=1}^{m}\prod_{l=1}^k(\frac{\exp(\theta_l^Tx^{(i)})}{\sum_{j=1}^k{\exp(\theta_j^Tx^{(i)})}})^{1\{y^{(i)}=l\}}
\]</span> 最终求解该似然函数的最优解可以得到整个模型的最优解及其对应的模型。</p></li>
</ol>
<h1 id="step-by-step-to-fulfill-softmax-classification-for-mnist-dataset">Step by Step to fulfill softmax classification for MNIST dataset</h1>
<h2 id="step1-load-sample-data-set">Step1: Load sample data set</h2>
<p>加载MNIST样本数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注：在执行上述代码时如果遇到file not gzip file错误，是由于下载的数据有问题。去修改源文件[Where tensorflow located]/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py, 将SOURCE_URL换成：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; SOURCE_URL = &apos;http://yann.lecun.com/exdb/mnist/&apos;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="step2-construct-glmsoftmax">Step2: Construct GLM(Softmax)</h2>
<p>构造假设函数（Softmax分类模型）：<span class="math inline">\(y = h_{\theta}(x) = softmax(W*x + b)\)</span></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">x = tf.placeholder(tf.float32, [None, 784])</span><br><span class="line">W = tf.Variable(tf.zeros([784, 10]))</span><br><span class="line">b = tf.Variable(tf.zeros([10]))</span><br><span class="line">y = tf.nn.softmax(tf.matmul(x, W) + b)</span><br></pre></td></tr></table></figure>
<h2 id="step3-train-glmsoftmax">Step3: Train GLM(Softmax)</h2>
<p>求解假设函数的最优值，构造最优化问题目标函数：<span class="math inline">\(H_{y_i^{‘}}(y) = H_{y_i^{‘}}(h_{\theta(x^{(i)}})\)</span></p>
<p>在此使用交叉熵作为目标函数，即：<span class="math inline">\(H_{y_i^{‘}}(y) = -\sum_i(y_i^{‘})\log(h_{\theta(x^{(i)})})\)</span></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_ = tf.placeholder(tf.float32, [None, 10])</span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>亦可以直接使用tensorflow提供的函数计算cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))<code>其中</code>y=W*x+b</p>
</blockquote>
<p>定义优化方法（梯度下降）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)</span><br></pre></td></tr></table></figure>
<p>Tensorflow执行优化过程（随机梯度下降）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">tf.global_variables_initializer().run()</span><br><span class="line">for _ in range(1000):</span><br><span class="line">  batch_xs, batch_ys = mnist.train.next_batch(100)</span><br><span class="line">  sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="step-4-模型评估">Step 4: 模型评估</h2>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">print(sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Frankie</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">categories</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Frankie</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
